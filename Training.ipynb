{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOFQfPQtLwvYJjFzhd2u4nc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mario-HN/Pytorch-DS/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: gere uma função que sirva para usar de data_loader para o dataset CIFAR-10, ela deve preparar tanto o treino como a validação\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def get_cifar10_data_loaders(batch_size, workers=4):\n",
        "  \"\"\"\n",
        "  Cria os data loaders para o dataset CIFAR-10.\n",
        "\n",
        "  Args:\n",
        "    batch_size: tamanho do batch.\n",
        "    workers: número de trabalhadores para carregar os dados.\n",
        "\n",
        "  Returns:\n",
        "    Uma tupla contendo os data loaders de treino e validação.\n",
        "  \"\"\"\n",
        "\n",
        "  # Transformações para os dados de treino e validação.\n",
        "  train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "  val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  # Carrega os datasets de treino e validação.\n",
        "  train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "  val_dataset = datasets.CIFAR10(root='./data', train=False, transform=val_transform)\n",
        "\n",
        "  # Cria os data loaders.\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=workers)\n",
        "  val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=workers)\n",
        "\n",
        "  return train_loader, val_loader\n"
      ],
      "metadata": {
        "id": "p6MuO762RWNq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: gere uma função para usar em treinamento de redes neurais usando pytorch framework. A função deve printar a acurácia de treino e de validação a cada final de época, é interessante o uso do tqdm para iteratividade do processo. A função deve receber parâmteros de scheduler (caso necessário), learning rate, batch size, etc. podemos chamá-la de fitness function\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "def fitness_function(model, train_loader, val_loader, scheduler=None, learning_rate=0.01, batch_size=64, epochs=10):\n",
        "    \"\"\"\n",
        "    Fitness function for training neural networks using PyTorch.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to train.\n",
        "        train_loader: The DataLoader for the training set.\n",
        "        val_loader: The DataLoader for the validation set.\n",
        "        scheduler: The scheduler to use for learning rate decay (optional).\n",
        "        learning_rate: The initial learning rate.\n",
        "        batch_size: The batch size.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Move model to the appropriate device\n",
        "    #model.to(0)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    # Train the model for the specified number of epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Train the model for one epoch\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        model.train()\n",
        "        for batch in tqdm(train_loader):\n",
        "            inputs, labels = batch\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs, labels = batch\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate the average losses\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Print the training and validation accuracy\n",
        "        train_accuracy = 100.0 * train_correct / train_total\n",
        "        val_accuracy = 100.0 * val_correct / val_total\n",
        "        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "        # Update the learning rate scheduler\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "3SnOiQytR8eH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: gere uma rede de convolução bem simples para teste, ela deve receber uma entrada de 32x32x3 pixels e no final ter um flatten ligado a uma softmax para classificacao\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(SimpleConvNet, self).__init__()\n",
        "\n",
        "    # Camadas de convolução\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "    # Camada de pooling\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    # Camada de flatten\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    # Camada totalmente conectada\n",
        "    self.fc = nn.Linear(in_features=32 * 8 * 8, out_features=10)\n",
        "\n",
        "    # Camada de softmax\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Camadas de convolução e pooling\n",
        "    x = self.pool(nn.ReLU()(self.conv1(x)))\n",
        "    x = self.pool(nn.ReLU()(self.conv2(x)))\n",
        "\n",
        "    # Camada de flatten\n",
        "    x = self.flatten(x)\n",
        "\n",
        "    # Camada totalmente conectada e softmax\n",
        "    x = self.softmax(self.fc(x))\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "o4XsKgdsTy5X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 64\n",
        "cnv_model = SimpleConvNet()\n",
        "train_loader, val_loader = get_cifar10_data_loaders(bs)"
      ],
      "metadata": {
        "id": "q54LFv52ULNS",
        "outputId": "7e0e6fab-ba0a-47e9-fe6b-1bad3ae0e362",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fitness_function(cnv_model, train_loader, val_loader, scheduler=None, learning_rate=0.01, batch_size=64, epochs=1)"
      ],
      "metadata": {
        "id": "wz61ULj_UG2b",
        "outputId": "87691b1c-37ef-40e1-a811-074edf43e7b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [00:37<00:00, 20.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train Loss: 2.1751, Train Accuracy: 27.57%, Val Loss: 2.1009, Val Accuracy: 35.51%\n"
          ]
        }
      ]
    }
  ]
}